# Quinn's summary of Why Should I Trust You: Explaining the Predictions of Any Classifier

> If the users do not trust a model or a prediction, they will not use it.

The million dollar word is "trust". When a user trusts a prediction, they feel ok about making a decision based on that prediction. When a user trusts a model, they feel ok deploying the model to production where, presumably, it will inform many decisions. In either case, it becomes crucial for a human to understand a model's behavior and not to just write it off as a black box. _Determining_ trust in individual predictions is crucial when the decisions made based on predictions are high stakes, with high cost of failure. When real world data differs from available validation dataset, _inspecting_ explanations can greatly assist metrics-driven model evaluation processes. Since the proposed procedure for building trust in models is a collection of the trust built in individual predictions, we will sometimes view model trust as a superset of prediction trust. 

The meaning of "explanation" employed in this paper is "presenting textual or visual artifacts that provide qualitative understanding of the relationship between the instance’s components (e.g. words in text, patches in an image) and the model’s prediction." In order to build trust, we want explanations to be _faithful_ and _intelligible_. One litmus test for trustworthiness is seeing that an explanation is _aligned_ with common sense or _priors_. Looking at individual examples is proposed as an augmentation to observing validation loss. 

Data leakage and dataset shift are two challenges ML engineers face that can be addressed by explanations. Explanation can give insight to measures that are harder to measure than accuracy like user retention or engagement. 

We want our explainers to be **interpretable**, exhibit **local fidelity**, be **model-agnostic**, and provide a **global perspective**. An explainer is interpretable when it provides a qualitative understanding between input variables and response. Complete faithfulness of a model is simply a description of the entire model, but a model exhibits local fidelity to another (unexplained) model when it's behavior is consistent in the vicinity of an instance with the unexplained model. An explainer is model-agnostic when it works just as well for linear models or decision trees. An explainer provides a global perspective when, although it filters down information, it's insights apply to the whole model. 

Enter LIME. **Local interpretable model-agnostic explanations** are an algorithm for producing explainers given a classifier. "The goal of LIME is to identify an interpretable model over the _interpretable representation_ that is _locally faithful_ to the classifier". First, what is the difference between a **feature** and an **interpretable data representation**? If you've ever done any feature engineering, you know that what the model sees toward the end of a pipeline and what the human is able to interpret at the input and output of a pipeline can be two different things. A human may need to know something like "does this token appear in the sentence" while a model needs something like an embedding. The paper chooses a binary vector as the interpretable representation for any data vector x. 

We are subject to a _fidelity-interpretability tradeoff_, where intuitively interpretation compresses information whereas fidelity does not. An _explanation_ is just a model from the class of potentially interpretable models. The model we're explaining can be grouped with an explanation and a proximity measure (defining when an instance is _local_ to another instance) and shoved into an _unfaithfulness_ measure, which specifically answers "how unfaithful is the explanation at approximating the model in the neighborhood described by the proximity measure?". We want to `argmin` over explanations the unfaithfulness measure plus the _complexity_ of the explanation. In other words, we minimize the locality-aware loss (between the model and the explanation) penalized by the complexity of the explanation. We proceed by _sampling_, we compare a perturbed sample (weighted by the proximity measure) as a _label_ that it got by being pushed through the model to the true label of the unperturbed sample. 

For example, we can let the proximity measure `pi_x (x') = exp(- D(x, x') ^ 2 / sigma ^ 2)` for appropriate distance function `D`, then the fidelity/loss is the `pi_x`-weighted squared error between an explanation's belief about perturbed from z sample z' and the model's (actual) belief about sample z. 

So an explanation is a model when you get by minimizing the complexity-penalized fidelity/loss, and you can evaluate different features in the explanation by observing, for instance, coefficients (when the explanation is drawn from the class of linear models). This gives us a method for explaining _instances_, or independent items from the dataset, because the method of getting loss and minimizing it is dependent on _proximity_ to a given example. The next thing we need to do is provide a means of evaluating (explaining) whole models. A procedure called _submodular pick_ steps in. The algorithm constructs an _explanation matrix_ that "represents local importance of the interpretable components for each instance", dimensions being the number of samples you want to inspect the explanation's behavior on times the dimensionality of your interpretable components (which, recall, are binary vectors). An example component of this explanation matrix, for a linear model, is the absolute value of a coefficient. "Intuitively, we want features that explain many different instances to have higher importance scores". It's important that the set of explanations picked are not redundant in the compnents they show the users. In other words, they should avoid selecting instances with similar explanations. A coverage metric can be given to formalize this, but the exact minimum is NP-hard, so we proceed greedily. 

In addition to introducing LIME, this paper performs two experiments, one with simulated users and the other with actual human users.

